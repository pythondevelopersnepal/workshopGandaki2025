{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c207332c",
   "metadata": {},
   "source": [
    "# Deep Learning Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf69f3e",
   "metadata": {},
   "source": [
    "### 1. Image Size: \n",
    "#### Image size refers to the dimensions (height and width) of an image, typically measured in pixels. For example, a 28x28 image has a height of 28 pixels and a width of 28 pixels.\n",
    "\n",
    "- Mathematical Dimension: An image can be represented as a multi-dimensional array (tensor). For a grayscale image, it's a 2D array: (height×width)\n",
    "\n",
    "- For a color image (like RGB), it's a 3D array: (height×width×channels) , where the number of channels is usually 3 (Red, Green, Blue).\n",
    "\n",
    "- Importance: The input layer of a deep learning model processing images needs to know the expected image size. All images in a batch are usually resized to a consistent size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a26087",
   "metadata": {},
   "source": [
    "### 2. Grayscale Image:\n",
    "#### A grayscale image contains only shades of gray, ranging from black to white. Each pixel in a grayscale image has a single intensity value representing its brightness.\n",
    "- Mathematical Dimension: A grayscale image is typically represented by a 2D array (matrix) where each element corresponds to the intensity of a pixel. The values usually range from 0 (black) to 255 (white), or can be normalized to a range like 0 to 1.\n",
    "- Contrast with Color Images: Color images, on the other hand, use multiple color channels (e.g., Red, Green, Blue) to represent a wider spectrum of colors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4744a",
   "metadata": {},
   "source": [
    "### 3. Normalize (Normalization):\n",
    "\n",
    "#### Normalization is a data preprocessing technique used to scale numerical data to a standard range, often between 0 and 1 or -1 and 1.\n",
    "- Mathematical Operation: For image data, a common normalization technique is to divide each pixel value by the maximum possible value (e.g., 255 for 8-bit grayscale images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd8587",
   "metadata": {},
   "source": [
    "### 4. Batch (Batch Size):\n",
    "\n",
    "- During the training of a deep learning model, the entire dataset is typically too large to process at once. Instead, the data is divided into smaller groups called batches. The batch size is the number of training examples processed in one forward and backward pass of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b2929",
   "metadata": {},
   "source": [
    "### 5. Epoch:\n",
    "\n",
    "- Definition: An epoch represents one complete pass of the entire training dataset through the deep learning model during the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c385a",
   "metadata": {},
   "source": [
    "### When to Use Deep Learning Algorithms?\n",
    "\n",
    "- Deep learning algorithms excel in situations where:\n",
    "\n",
    "1. Large Amounts of Data are Available: Deep learning models typically require significant amounts of labeled data to learn complex patterns effectively.\n",
    "2. Complex Patterns and Abstractions: The underlying relationships in the data are intricate and hierarchical, making it difficult for traditional machine learning algorithms to capture them.\n",
    "3. Feature Engineering is Difficult or Time-Consuming: Deep learning models can automatically learn relevant features from raw data, reducing the need for manual feature engineering.   \n",
    "4. High Accuracy is Required: In many domains, deep learning has achieved state-of-the-art results, often surpassing traditional methods in accuracy.\n",
    "5. Specific Problem Domains: Deep learning has shown remarkable success in areas like:\n",
    "- Computer Vision: Image classification, object detection, image segmentation.\n",
    "- Natural Language Processing (NLP): Text classification, machine translation, sentiment analysis.\n",
    "- Speech Recognition: Converting spoken language into text.\n",
    "- Time Series Analysis: Forecasting, anomaly detection.\n",
    "- Recommendation Systems: Predicting user preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac5f62",
   "metadata": {},
   "source": [
    "### How Much Data is \"Less Data\" for Deep Learning?\n",
    "\n",
    "- Defining \"less data\" is relative to the complexity of the problem and the capacity of the deep learning model. However, some general guidelines can be considered:\n",
    "\n",
    "1. For very simple tasks (e.g., basic binary classification with clear features): A few hundred to a few thousand labeled examples might be sufficient for some traditional machine learning models. Deep learning might still work but could be overkill and prone to overfitting with such small datasets.\n",
    "\n",
    "2. For moderately complex tasks (e.g., image classification with a few classes and relatively distinct features): Thousands to tens of thousands of labeled examples per class are often a starting point for training a reasonably good deep learning model.\n",
    "\n",
    "3. For highly complex tasks (e.g., high-resolution image classification with many classes, natural language understanding, video analysis): Deep learning models often require hundreds of thousands to millions of labeled examples to achieve good generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9020213",
   "metadata": {},
   "source": [
    "### Overfitting happens when your model learns the training data too well — including its noise and outliers. It performs well on the training set but poorly on unseen/test data.\n",
    "\n",
    "Key Symptoms:\n",
    "\n",
    "✅ High training accuracy\n",
    "\n",
    "❌ Low validation/test accuracy\n",
    "\n",
    "The model \"memorizes\" instead of \"generalizing\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3419368",
   "metadata": {},
   "source": [
    "### Underfitting occurs when your model is too simple to capture the underlying patterns in the data — it performs poorly on both training and test sets.\n",
    "\n",
    "Key Symptoms:\n",
    "\n",
    "❌ Low training accuracy\n",
    "\n",
    "❌ Low validation accuracy\n",
    "\n",
    "The model can't learn enough from the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
